import os
import torch
import torch.nn as nn
from transformers import PreTrainedModel
from utils.patch_encoder import bchw_to_pixel_values  # ç¡®ä¿è¯¥å‡½æ•°æ— çŠ¶æ€ï¼ˆçº¯å‡½æ•°æœ€ä½³ï¼‰
from transformers import PretrainedConfig
from torchvision import transforms
class HierarchicalModelConfig(PretrainedConfig):
    model_type = "hierarchical_vlm"

    def __init__(
        self,
        vlm_config=None,      # ä¸» VLM çš„ configï¼ˆå¦‚ Qwen2VLConfigï¼‰
        filter_trainable=True,
        vlm_trainable=False,
        simulate_trainable=False,
        **kwargs
    ):
        super().__init__(**kwargs)
        self.vlm_config = vlm_config
        self.filter_trainable = filter_trainable
        self.simulate_trainable = simulate_trainable
        self.vlm_trainable = vlm_trainable

class HierarchicalModel(PreTrainedModel):
    config_class = HierarchicalModelConfig
    base_model_prefix = "hierarchical_model"
    _no_split_modules = ["ColorFilter", "CVDSimulator"]  # è‹¥æœ‰å¤§æ¨¡å—ï¼Œé˜²æ­¢ DDP split

    def __init__(self, vlm_model, color_filter, cvd_simulator, config=None):
        if config is None:
            config = HierarchicalModelConfig(
                        vlm_config=None,
                        filter_trainable=True,
                        vlm_trainable=False,
                        simulate_trainable=False,
                    )
        super().__init__(config)
        self.config = config
        
        # ä¸»æ¨¡å‹ï¼ˆå¦‚ Qwen2VLForConditionalGenerationï¼‰
        self.vlm_model = vlm_model
        
        # å¯å­¦ä¹ æ¨¡å—ï¼ˆç¡®ä¿æ˜¯ nn.Moduleï¼‰
        # self.color_filter = color_filter
        class debugFilter(nn.Module):
            def forward(self,x):
                return x
        self.color_filter = debugFilter()
        self.cvd_simulator = cvd_simulator
        self.trans_compose_forward = transforms.Compose(
                [transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])]
            )
        # å¯é€‰ï¼šå†»ç»“éƒ¨åˆ†æ¨¡å—
        if not config.filter_trainable:
            for p in self.color_filter.parameters():
                p.requires_grad = False
        if not config.simulate_trainable:
            for p in self.cvd_simulator.parameters():
                p.requires_grad = False
        if not config.vlm_trainable:
            for p in self.vlm_model.parameters():
                p.requires_grad = False

    @classmethod
    def from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs):
        # è‹¥éœ€ä»å·²æœ‰ checkpoint åŠ è½½ï¼Œå¯è‡ªå®šä¹‰ï¼›å¦åˆ™ç”¨é»˜è®¤å³å¯
        return super().from_pretrained(pretrained_model_name_or_path, *model_args, **kwargs)

    def forward(
        self,
        ori_images=None,       # [B, C, H, W]ï¼ŒåŸå§‹è¾“å…¥å›¾åƒ â€”â€” ä»…ç”¨äºé¢„å¤„ç†
        input_ids=None,
        attention_mask=None,
        labels=None,           # â† SFTTrainer å¿…éœ€ï¼
        pixel_values=None,     # å¯é€‰ï¼šè‹¥å·²æœ‰é¢„å¤„ç†å›¾åƒå¯è·³è¿‡
        **kwargs,
    ):
        # âœ… å…³é”®ï¼šåŠ¨æ€å¤„ç†è®¾å¤‡/ç±»å‹è¿ç§»ï¼ˆé¿å…ç¡¬ç¼–ç ï¼‰
        if ori_images is not None:
            # è‡ªåŠ¨è¿ç§»è®¾å¤‡/dtypeï¼ˆå…¼å®¹ accelerateï¼‰
            ori_images = ori_images.to(dtype=self.vlm_model.dtype)
            
            # é¢„å¤„ç†æµæ°´çº¿
            processed_image = self.color_filter(ori_images)
            processed_image = self.cvd_simulator(processed_image)
            processed_image = self.trans_compose_forward(processed_image)
            # ç¼–ç ä¸º VLM æ‰€éœ€ tokens
            pixel_values, image_grid_thw = self._encode_images(processed_image)
            kwargs["pixel_values"] = pixel_values
            if "image_grid_thw" not in kwargs:
                kwargs["image_grid_thw"] = image_grid_thw

        # è½¬å‘ç»™ä¸» VLMï¼ˆç¡®ä¿ labels é€ä¼ ï¼‰
        outputs = self.vlm_model(
            input_ids=input_ids,
            attention_mask=attention_mask,
            labels=labels,      # â† è®¡ç®— loss çš„å…³é”®ï¼
            **kwargs
        )
        outputs["ori_images"] = ori_images
        outputs["enhance_images"] = processed_image
        return outputs 

    @torch.no_grad()
    def generate(self, ori_images=None, **kwargs):
        if ori_images is not None:
            ori_images = ori_images.to(dtype=self.vlm_model.dtype)
            processed = self.color_filter(ori_images)
            processed = self.cvd_simulator(processed)
            pixel_values, grid = self._encode_images(processed)
            kwargs["pixel_values"] = pixel_values
            kwargs.setdefault("image_grid_thw", grid)
        return self.vlm_model.generate(**kwargs)

    def _encode_images(self, images):
        # ç¡®ä¿è¯¥å‡½æ•°æ˜¯çº¯å‡½æ•°ï¼ˆæ— å‚æ•°ï¼‰ï¼Œæˆ–å°†å…¶å°è£…ä¸º nn.Module
        return bchw_to_pixel_values(
            images,
            patch_size=16,
            temporal_patch_size=2,
            merge_size=2
        )

    def save_pretrained(
        self,
        save_directory: str,
        save_full_model: bool = False,   # æ–°å¢å¼€å…³
        **kwargs
    ):
        os.makedirs(save_directory, exist_ok=True)

        if save_full_model:
            # é»˜è®¤è¡Œä¸ºï¼šä¿å­˜å®Œæ•´æ¨¡å‹ï¼ˆç”¨äºæ¢å¤è®­ç»ƒï¼‰
            super().save_pretrained(save_directory, **kwargs)
            # å¯é€‰ï¼šåŒæ—¶ä¿å­˜è½»é‡ç‰ˆï¼ˆè§ä¸‹æ–¹ï¼‰
            self._save_color_filter_only(
                os.path.join(save_directory, "color_filter_only")
            )
        else:
            # æ˜¾å¼è¦æ±‚åªä¿å­˜ color_filter
            self._save_color_filter_only(save_directory)

    def _save_color_filter_only(self, save_directory: str):
        """å†…éƒ¨æ–¹æ³•ï¼šåªä¿å­˜ color_filter"""
        os.makedirs(save_directory, exist_ok=True)
        
        # 1. ä¿å­˜ç²¾ç®€ configï¼ˆå¯é€‰ï¼‰
        from transformers import PretrainedConfig
        filter_config = PretrainedConfig(
            model_type="color_filter",
            original_model=self.config._name_or_path if hasattr(self.config, "_name_or_path") else "unknown"
        )
        filter_config.save_pretrained(save_directory)
        
        # 2. ä¿å­˜ color_filter state_dict
        state_dict = {
            f"color_filter.{k}": v 
            for k, v in self.color_filter.state_dict().items()
        }
        torch.save(state_dict, os.path.join(save_directory, "pytorch_model.bin"))
        
        # 3. ä¿å­˜ tokenizerï¼ˆè‹¥éœ€ç”Ÿæˆï¼‰
        if hasattr(self, "tokenizer") and self.tokenizer is not None:
            self.tokenizer.save_pretrained(save_directory)
        
        print(f"ğŸ¨ Saved color_filter only to {save_directory}")
    @property
    def device(self):
        return next(self.parameters()).device

    @property
    def dtype(self):
        return next(self.parameters()).dtype
    def gradient_checkpointing_enable(self, gradient_checkpointing_kwargs=None):
        """
        Enable gradient checkpointing for the underlying VLM.
        """
        if hasattr(self.vlm_model, "gradient_checkpointing_enable"):
            self.vlm_model.gradient_checkpointing_enable(
                gradient_checkpointing_kwargs=gradient_checkpointing_kwargs
            )
        else:
            raise ValueError(
                f"Underlying model {type(self.vlm_model)} does not support gradient checkpointing."
            )

    def gradient_checkpointing_disable(self):
        """
        Disable gradient checkpointing for the underlying VLM.
        """
        if hasattr(self.vlm_model, "gradient_checkpointing_disable"):
            self.vlm_model.gradient_checkpointing_disable()
        else:
            # å¦‚æœä¸æ”¯æŒï¼Œé™é»˜å¿½ç•¥ï¼ˆæˆ– warningï¼‰
            pass

    @property
    def supports_gradient_checkpointing(self):
        # è®© Trainer çŸ¥é“æœ¬æ¨¡å‹â€œæ”¯æŒâ€ï¼ˆå®é™…æ˜¯åº•å±‚æ”¯æŒï¼‰
        return getattr(self.vlm_model, "supports_gradient_checkpointing", False)

    # å¯é€‰ï¼šåŠ ä¸€ä¸ªå¿«æ·å±æ€§ï¼ˆæŸäº›ä»£ç ä¼šæ£€æŸ¥ï¼‰
    @property
    def gradient_checkpointing(self):
        return getattr(self.vlm_model, "gradient_checkpointing", False)