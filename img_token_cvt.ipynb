{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c2762e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f003e166f816488d8e72ddcdd7b958f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForImageTextToText, AutoProcessor, DataCollatorForSeq2Seq\n",
    "from trl.trainer.sft_trainer import DataCollatorForVisionLanguageModeling\n",
    "from processor.color_enhance_collator import ColorSensitiveCollator\n",
    "from processor.color_simulate_collator import ColorSimulateCollator\n",
    "from transformers import AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "import torchvision\n",
    "from matplotlib import pyplot as plt\n",
    "from datasets import Dataset\n",
    "\n",
    "MODEL_NAME = \"/root/autodl-tmp/model\"  # 或任何支持 swift 的多模态模型\n",
    "DATA_PATH = \"/root/color_150k.json\"  # SFT 数据：包含 {\"instruction\": ..., \"output\": ...}\n",
    "OUTPUT_DIR = \"./output_color_sensitive\"\n",
    "CVD_TYPE = \"Deuteranomaly\"\n",
    "CVD_SEVERITY = 1.0\n",
    "\n",
    "# ===== 加载模型与processor =====\n",
    "model = AutoModelForImageTextToText.from_pretrained(\n",
    "    MODEL_NAME, dtype=\"auto\", device_map=\"auto\"\n",
    ")\n",
    "dtype = model.dtype\n",
    "device = model.device\n",
    "processor = AutoProcessor.from_pretrained(MODEL_NAME)\n",
    "collator = ColorSimulateCollator(processor,cvd_type=\"nt\")\n",
    "SYS_PROMPT = \"\"\" You are a color blind with limited perception on image. \n",
    "However, you can still guess the color from your experience. \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d51d6e8-6db7-48cd-a988-70ad864b30c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_image_dataset:\n",
      " {'image': '/root/autodl-tmp/images/train2017/000000033471.jpg', 'messages': [{'content': [{'image': None, 'text': ' You are a color blind with limited perception on image. \\nHowever, you can still guess the color from your experience. ', 'type': 'text'}], 'role': 'system'}, {'content': [{'image': None, 'text': '<image>\\\\n', 'type': 'text'}, {'image': '/root/autodl-tmp/images/train2017/000000033471.jpg', 'text': None, 'type': 'image'}], 'role': 'user'}, {'content': [{'image': None, 'text': 'Describe the image.', 'type': 'text'}], 'role': 'user'}]}\n",
      "input_text with special tokens: <|im_start|>system\n",
      " You are a color blind with limited perception on image. \n",
      "However, you can still guess the color from your experience. <|im_end|>\n",
      "<|im_start|>user\n",
      "<|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|im_end|>\n",
      "<|im_start|>user\n",
      "<|vision_start|><|image_pad|><|vision_end|><|im_end|>\n",
      "\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 1 is out of bounds for dimension 0 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 29\u001b[39m\n\u001b[32m     27\u001b[39m input_text = processor.apply_chat_template(sample_message[\u001b[32m0\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m], tokenize=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m     28\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33minput_text with special tokens: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_text\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m inputs = \u001b[43mprocessor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43minput_text\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     31\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     32\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[43m)\u001b[49m.to(\u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TextStreamer\n\u001b[32m     37\u001b[39m text_streamer = TextStreamer(processor, skip_prompt = \u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/transformers/models/qwen3_vl/processing_qwen3_vl.py:191\u001b[39m, in \u001b[36mQwen3VLProcessor.__call__\u001b[39m\u001b[34m(self, images, text, videos, **kwargs)\u001b[39m\n\u001b[32m    189\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(text)):\n\u001b[32m    190\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m.image_token \u001b[38;5;129;01min\u001b[39;00m text[i]:\n\u001b[32m--> \u001b[39m\u001b[32m191\u001b[39m         num_image_tokens = \u001b[43mimage_grid_thw\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m.prod() // merge_length\n\u001b[32m    192\u001b[39m         text[i] = text[i].replace(\u001b[38;5;28mself\u001b[39m.image_token, \u001b[33m\"\u001b[39m\u001b[33m<|placeholder|>\u001b[39m\u001b[33m\"\u001b[39m * num_image_tokens, \u001b[32m1\u001b[39m)\n\u001b[32m    193\u001b[39m         index += \u001b[32m1\u001b[39m\n",
      "\u001b[31mIndexError\u001b[39m: index 1 is out of bounds for dimension 0 with size 1"
     ]
    }
   ],
   "source": [
    "sample_message = [\n",
    "{    \n",
    "    \"image\": \"/root/autodl-tmp/images/train2017/000000033471.jpg\",\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\":\"system\",\n",
    "            \"content\":[{\"type\":\"text\",\"text\":SYS_PROMPT}]\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": r\"<image>\\n\"},{\"type\": \"image\",  \"image\": \"/root/autodl-tmp/images/train2017/000000033471.jpg\"}]\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": \"Describe the image.\"}]\n",
    "        },\n",
    "        # {\n",
    "        #     \"role\": \"assistant\",\n",
    "        #     \"content\": [{\"type\": \"text\", \"text\": \"The image is a street scene with a car and a person.\"}]\n",
    "        # }\n",
    "    ]\n",
    "}\n",
    "]\n",
    "sample_image_dataset = Dataset.from_list(sample_message)\n",
    "print(f\"sample_image_dataset:\\n {sample_image_dataset[0]}\")\n",
    "image = sample_message[0][\"image\"]\n",
    "input_text = processor.apply_chat_template(sample_message[0][\"messages\"], tokenize=False)\n",
    "print(f\"input_text with special tokens: {input_text}\")\n",
    "inputs = processor(\n",
    "    text=[input_text], \n",
    "    images=[image], \n",
    "    padding=True,\n",
    "    return_tensors = \"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(processor, skip_prompt = True)\n",
    "_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 256,\n",
    "                   use_cache = True, temperature = 1.5, min_p = 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf31802",
   "metadata": {},
   "source": [
    "## 构建两条信息，图像尺寸不一定一样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb34b96d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(156, 155)\n",
      "(640, 480)\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "image = \"./traffic_light.png\"\n",
    "dataset_message = [\n",
    "    {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\":\"system\",\"content\":[{\"type\":\"text\",\"text\":SYS_PROMPT}]\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [{\"type\": \"text\", \"text\": \"Describe the image.\"},{\"type\": \"image\", \"image\": \"./parrots.png\"}]\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": [{\"type\": \"text\", \"text\": \"The image describes three parrots, colored yellow, red and blue.\"}]\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "    # {\n",
    "    #     \"messages\": [\n",
    "    #         {\n",
    "    #             \"role\":\"system\",\"content\":[{\"type\":\"text\",\"text\":SYS_PROMPT}]\n",
    "    #         },\n",
    "    #         {\n",
    "    #             \"role\": \"user\",\n",
    "    #             \"content\": [{\"type\": \"text\", \"text\": \"What are the colors of the bus in the image?(pay attention to color especially). \"},\n",
    "    #                         {\"type\": \"image\", \"image\": \"/root/autodl-tmp/images/train2017/000000033471.jpg\"}]\n",
    "    #         },\n",
    "    #         {\n",
    "    #             \"role\": \"assistant\",\n",
    "    #             \"content\": [{\"type\": \"text\", \"text\": \"The bus in the image is white and red.\"}]\n",
    "    #         }\n",
    "    #     ]\n",
    "    # },\n",
    "]\n",
    "\n",
    "# input_text = processor.apply_chat_template(dataset_message, add_generation_prompt = True)\n",
    "# inputs = processor(\n",
    "#     image,\n",
    "#     input_text,\n",
    "#     add_special_tokens = False,\n",
    "#     return_tensors = \"pt\",\n",
    "# ).to(\"cuda\")\n",
    "\n",
    "# # debug\n",
    "# print(inputs)\n",
    "\n",
    "# from transformers import TextStreamer\n",
    "# text_streamer = TextStreamer(processor, skip_prompt = True)\n",
    "# _ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 256,\n",
    "#                    use_cache = True, temperature = 1.5, min_p = 0.1)\n",
    "print(Image.open(\"./parrots.png\").size)\n",
    "print(Image.open(\"/root/autodl-tmp/images/train2017/000000033471.jpg\").size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7776f12c",
   "metadata": {},
   "source": [
    "## 检查初步编码的结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49f5e0f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list_messages = [dataset_message[i][\"messages\"] for i in range(len(dataset_message))]\n",
    "text = processor.apply_chat_template(list_messages, tokenize=False, add_generation_prompt=True)\n",
    "images, videos, video_kwargs = process_vision_info(list_messages, return_video_kwargs=True)\n",
    "inputs = processor(text=text, images=images, videos=videos, padding=True, return_tensors=\"pt\", **video_kwargs)\n",
    "print(inputs[\"pixel_values\"].shape)\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81621864",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "out = collator(dataset_message)\n",
    "# print(out)\n",
    "img_pixels = out[\"pixel_values\"]\n",
    "print(img_pixels.shape)\n",
    "img_thw = out[\"image_grid_thw\"]\n",
    "print(img_thw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee9a1261",
   "metadata": {},
   "source": [
    "## 尝试推理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b6d887",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(processor, skip_prompt = True)\n",
    "_ = model.generate(**out, streamer = text_streamer, max_new_tokens = 256,\n",
    "                   use_cache = True, temperature = 1.5, min_p = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48fd0bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from typing import Optional, Tuple, Union\n",
    "\n",
    "# def invert_patches_to_image(\n",
    "#     pixel_values: torch.Tensor,\n",
    "#     grid_shape: Union[Tuple[int, int, int], torch.Tensor],\n",
    "#     patch_size: int,\n",
    "#     temporal_patch_size: int = 1,\n",
    "#     image_mean: Optional[Union[float, list, torch.Tensor]] = None,\n",
    "#     image_std: Optional[Union[float, list, torch.Tensor]] = None,\n",
    "#     rescale_factor: float = 255.0,\n",
    "#     do_rescale: bool = True,\n",
    "#     do_denormalize: bool = True,\n",
    "#     output_format: str = \"CHW\",  # \"CHW\" or \"HWC\" or \"TCHW\" for video\n",
    "#     dtype: torch.dtype = torch.float32,\n",
    "# ) -> torch.Tensor:\n",
    "#     \"\"\"\n",
    "#     Differentiable inverse of Qwen's patch flattening process.\n",
    "    \n",
    "#     Args:\n",
    "#         pixel_values: (..., N, D), where N = grid_t * grid_h * grid_w, D = C * T * P^2\n",
    "#                       e.g., (B, N, D) or (N, D)\n",
    "#         grid_shape: (grid_t, grid_h, grid_w) — tuple or int tensor of shape (3,)\n",
    "#         patch_size: spatial patch size P\n",
    "#         temporal_patch_size: T (usually 1 for images)\n",
    "#         image_mean/std: used for denormalization (broadcastable to [C])\n",
    "#         output_format: \n",
    "#             - \"CHW\": return (..., C, H, W)\n",
    "#             - \"HWC\": return (..., H, W, C)\n",
    "#             - \"TCHW\": for video, return (..., T_out, C, H, W) with T_out = grid_t\n",
    "#         dtype: output dtype (e.g., torch.float32 for grad, torch.uint8 not allowed)\n",
    "\n",
    "#     Returns:\n",
    "#         Reconstructed image(s), same batch shape as pixel_values[:-2]\n",
    "#         Shape:\n",
    "#           - \"CHW\":  (..., C, grid_h * P, grid_w * P)\n",
    "#           - \"TCHW\": (..., grid_t, C, grid_h * P, grid_w * P)\n",
    "#     \"\"\"\n",
    "#     # --- 1. Parse shapes ---\n",
    "#     if isinstance(grid_shape, tuple):\n",
    "#         grid_t, grid_h, grid_w = grid_shape\n",
    "#     else:\n",
    "#         grid_t, grid_h, grid_w = grid_shape.unbind(-1) if grid_shape.ndim > 0 else grid_shape.tolist()\n",
    "\n",
    "#     *batch_shape, N, D = pixel_values.shape\n",
    "#     C = 3  # assume RGB\n",
    "#     expected_D = C * temporal_patch_size * (patch_size ** 2)\n",
    "#     assert D == expected_D, f\"Expected D={expected_D}, got {D}\"\n",
    "\n",
    "#     total_patches = grid_t * grid_h * grid_w\n",
    "#     assert N == total_patches, f\"Number of patches mismatch: {N} vs {total_patches}\"\n",
    "\n",
    "#     # --- 2. Reshape to (..., grid_t, grid_h, grid_w, C, T, P, P) ---\n",
    "#     new_shape = (*batch_shape, grid_t, grid_h, grid_w, C, temporal_patch_size, patch_size, patch_size)\n",
    "#     patches = pixel_values.reshape(new_shape)  # [..., T, Hg, Wg, C, Tp, P, P]\n",
    "\n",
    "#     # --- 3. Rearrange to [..., T, Hg, Wg, P, P, C, Tp] then collapse spatial dims ---\n",
    "#     # Permute to: [..., T, Hg, Wg, P, P, C, Tp]\n",
    "#     permute_order = tuple(range(len(batch_shape))) + (-6, -5, -4, -2, -1, -7, -3)  # T, Hg, Wg, P, P, C, Tp\n",
    "#     patches = patches.permute(*permute_order)  # [..., T, Hg, Wg, P, P, C, Tp]\n",
    "\n",
    "#     # Collapse (T * Tp) into effective temporal dim\n",
    "#     if temporal_patch_size > 1:\n",
    "#         # -> [..., T * Tp, Hg, Wg, P, P, C]\n",
    "#         patches = patches.flatten(-7, -6)  # flatten first two dims: T and Tp\n",
    "#         out_t = grid_t * temporal_patch_size\n",
    "#     else:\n",
    "#         # -> [..., T, Hg, Wg, P, P, C]\n",
    "#         patches = patches.squeeze(-1)  # remove Tp dim\n",
    "#         out_t = grid_t\n",
    "\n",
    "#     # Now: [..., T_eff, Hg, Wg, P, P, C]\n",
    "\n",
    "#     # --- 4. Use fold-like stitching via view + permute + reshape ---\n",
    "#     # Goal: combine (Hg, P) -> H = Hg * P, same for W\n",
    "#     # Shape: [..., T_eff, Hg, Wg, P, P, C]\n",
    "#     patches = patches.permute(\n",
    "#         *range(len(batch_shape)),  # batch\n",
    "#         -6,  # T_eff\n",
    "#         -1,  # C\n",
    "#         -5, -3,  # Hg, P -> to be merged as H\n",
    "#         -4, -2   # Wg, P -> to be merged as W\n",
    "#     )  # [..., T_eff, C, Hg, P, Wg, P]\n",
    "\n",
    "#     H, W = grid_h * patch_size, grid_w * patch_size\n",
    "#     if out_t == 1:\n",
    "#         # Image case: drop T dim\n",
    "#         patches = patches.squeeze(-6)  # [..., C, Hg, P, Wg, P]\n",
    "#         image = patches.reshape(*batch_shape, C, H, W)  # [..., C, H, W]\n",
    "#         if output_format == \"HWC\":\n",
    "#             image = image.permute(*range(len(batch_shape)), 1, 2, 0)  # [..., H, W, C]\n",
    "#     else:\n",
    "#         # Video case\n",
    "#         image = patches.reshape(*batch_shape, out_t, C, H, W)  # [..., T, C, H, W]\n",
    "#         if output_format == \"TCHW\":\n",
    "#             pass  # already [..., T, C, H, W]\n",
    "#         elif output_format == \"CHW\":\n",
    "#             # collapse T into batch, or error?\n",
    "#             raise ValueError(\"output_format='CHW' not supported for video (grid_t > 1). Use 'TCHW'.\")\n",
    "#         elif output_format == \"HWC\":\n",
    "#             image = image.permute(*range(len(batch_shape)), 0, 2, 3, 1)  # [..., T, H, W, C]\n",
    "\n",
    "#     # --- 5. Post-processing (differentiable!) ---\n",
    "#     if do_denormalize and image_mean is not None and image_std is not None:\n",
    "#         if isinstance(image_mean, (float, int)):\n",
    "#             image_mean = torch.tensor([image_mean] * 3, device=image.device, dtype=image.dtype)\n",
    "#         elif isinstance(image_mean, (list, tuple)):\n",
    "#             image_mean = torch.tensor(image_mean, device=image.device, dtype=image.dtype)\n",
    "#         if isinstance(image_std, (float, int)):\n",
    "#             image_std = torch.tensor([image_std] * 3, device=image.device, dtype=image.dtype)\n",
    "#         elif isinstance(image_std, (list, tuple)):\n",
    "#             image_std = torch.tensor(image_std, device=image.device, dtype=image.dtype)\n",
    "#         # broadcast to [C, 1, 1] or [1, C, 1, 1] etc.\n",
    "#         mean = image_mean.view(*([1] * (image.ndim - 3)), C, 1, 1)\n",
    "#         std = image_std.view(*([1] * (image.ndim - 3)), C, 1, 1)\n",
    "#         image = image * std + mean\n",
    "\n",
    "#     if do_rescale:\n",
    "#         image = image * rescale_factor\n",
    "\n",
    "#     return image.to(dtype)\n",
    "\n",
    "import torch\n",
    "from typing import Tuple, Optional, Union, List\n",
    "\n",
    "def _invert_single(\n",
    "    flatten_patches_i: torch.Tensor,\n",
    "    grid_t: int,\n",
    "    grid_h: int,\n",
    "    grid_w: int,\n",
    "    patch_size: int,\n",
    "    temporal_patch_size: int,\n",
    "    merge_size: int,\n",
    "    channel: int,\n",
    "    do_unnormalize: bool = False,\n",
    "    image_mean: Optional[Union[float, list, torch.Tensor]] = None,\n",
    "    image_std: Optional[Union[float, list, torch.Tensor]] = None,\n",
    "    do_unrescale: bool = False,\n",
    "    rescale_factor: Optional[float] = None,\n",
    "    output_data_format: str = \"channels_first\",\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    可导版：只处理单张图对应的 flatten_patches_i，返回 (num_frames, C, H, W)（channels_first）\n",
    "    \"\"\"\n",
    "    # flatten_patches_i: (P_i, S)\n",
    "    P_i, S = flatten_patches_i.shape\n",
    "    expected_P_i = grid_t * grid_h * grid_w\n",
    "    if P_i != expected_P_i:\n",
    "        raise ValueError(f\"P_i ({P_i}) != grid_t*grid_h*grid_w ({expected_P_i})\")\n",
    "\n",
    "    expected_S = channel * temporal_patch_size * patch_size * patch_size\n",
    "    if S != expected_S:\n",
    "        raise ValueError(f\"S ({S}) != channel*temporal_patch_size*patch_size*patch_size ({expected_S})\")\n",
    "\n",
    "    if grid_h % merge_size != 0 or grid_w % merge_size != 0:\n",
    "        raise ValueError(\"grid_h and grid_w must be divisible by merge_size\")\n",
    "\n",
    "    gh_div_ms = grid_h // merge_size\n",
    "    gw_div_ms = grid_w // merge_size\n",
    "\n",
    "    # reshape back to 9-dim as forward built before transpose\n",
    "    arr = flatten_patches_i.view(\n",
    "        grid_t,\n",
    "        gh_div_ms,\n",
    "        gw_div_ms,\n",
    "        merge_size,\n",
    "        merge_size,\n",
    "        channel,\n",
    "        temporal_patch_size,\n",
    "        patch_size,\n",
    "        patch_size,\n",
    "    )\n",
    "\n",
    "    # inverse transpose of forward perm=[0,3,6,4,7,2,1,5,8] -> perm_inv=[0,6,5,1,3,7,2,4,8]\n",
    "    perm_inv = [0, 6, 5, 1, 3, 7, 2, 4, 8]\n",
    "    arr = arr.permute(perm_inv)\n",
    "\n",
    "    # reorder to (grid_t, channel, temporal_patch_size, gh_div_ms, merge_size, patch_size, gw_div_ms, merge_size, patch_size)\n",
    "    arr = arr.permute(0, 2, 1, 3, 4, 5, 6, 7, 8)\n",
    "\n",
    "    H = grid_h * patch_size\n",
    "    W = grid_w * patch_size\n",
    "\n",
    "    # merge spatial dims -> (grid_t, channel, temporal_patch_size, H, W)\n",
    "    arr = arr.reshape(grid_t, channel, temporal_patch_size, H, W)\n",
    "\n",
    "    # collapse time dims -> (num_frames, channel, H, W)\n",
    "    num_frames = grid_t * temporal_patch_size\n",
    "    arr = arr.permute(0, 2, 1, 3, 4).reshape(num_frames, channel, H, W)\n",
    "    images = arr  # channels_first\n",
    "\n",
    "    # optional unnormalize / unrescale\n",
    "    if do_unnormalize:\n",
    "        if image_std is None or image_mean is None:\n",
    "            raise ValueError(\"image_std and image_mean must be provided when do_unnormalize=True\")\n",
    "        mean = torch.as_tensor(image_mean, dtype=images.dtype, device=images.device)\n",
    "        std = torch.as_tensor(image_std, dtype=images.dtype, device=images.device)\n",
    "        mean = mean.view(1, -1, 1, 1) if mean.ndim == 1 else mean.view(1, 1, 1, 1)\n",
    "        std = std.view(1, -1, 1, 1) if std.ndim == 1 else std.view(1, 1, 1, 1)\n",
    "        images = images * std + mean\n",
    "\n",
    "    if do_unrescale:\n",
    "        if rescale_factor is None:\n",
    "            raise ValueError(\"rescale_factor must be provided when do_unrescale=True\")\n",
    "        images = images / rescale_factor\n",
    "\n",
    "    if output_data_format == \"channels_last\":\n",
    "        images = images.permute(0, 2, 3, 1)  # (num_frames, H, W, C)\n",
    "\n",
    "    return images\n",
    "\n",
    "\n",
    "def invert_patches_to_images_batch(\n",
    "    flatten_patches: torch.Tensor,\n",
    "    image_grid_thw: torch.Tensor,\n",
    "    patch_size: int,\n",
    "    temporal_patch_size: int,\n",
    "    merge_size: int,\n",
    "    channel: int = 3,\n",
    "    *,\n",
    "    do_unnormalize: bool = False,\n",
    "    image_mean: Optional[Union[float, list, torch.Tensor]] = None,\n",
    "    image_std: Optional[Union[float, list, torch.Tensor]] = None,\n",
    "    do_unrescale: bool = False,\n",
    "    rescale_factor: Optional[float] = None,\n",
    "    output_data_format: str = \"channels_first\",\n",
    "    return_as: str = \"padded\",  # \"list\" or \"padded\"\n",
    "):\n",
    "    \"\"\"\n",
    "    处理 batch 中每张图可能不同 grid 的情况（可导实现）\n",
    "\n",
    "    Args:\n",
    "        flatten_patches: (P_total, S)\n",
    "        image_grid_thw: (N, 3) Long or Int tensor, 每行 [grid_t, grid_h, grid_w]\n",
    "        patch_size, temporal_patch_size, merge_size, channel: 全局参数\n",
    "        return_as: \"list\" 返回 list of tensors; \"padded\" 返回 (N, max_frames, C, max_H, max_W) and mask\n",
    "    Returns:\n",
    "        if return_as == \"list\": List[Tensor(num_frames_i, C, H_i, W_i)]\n",
    "        else: (padded_tensor, mask) where\n",
    "            padded_tensor: (N, max_frames, C, max_H, max_W)\n",
    "            mask: bool Tensor (N, max_frames) indicating valid frames per sample\n",
    "    \"\"\"\n",
    "    if flatten_patches.ndim != 2:\n",
    "        raise ValueError(\"flatten_patches must be 2D (P_total, S)\")\n",
    "    if image_grid_thw.ndim != 2 or image_grid_thw.shape[1] != 3:\n",
    "        raise ValueError(\"image_grid_thw must be (N,3) tensor with [grid_t, grid_h, grid_w] per row\")\n",
    "\n",
    "    device = flatten_patches.device\n",
    "    dtype = flatten_patches.dtype\n",
    "\n",
    "    N = image_grid_thw.shape[0]\n",
    "    grid_vals = image_grid_thw.to(torch.long).cpu().numpy()  # small, fine to use numpy here for ints\n",
    "    # compute per-image P_i and check total matches\n",
    "    Pis = []\n",
    "    for (gt, gh, gw) in grid_vals:\n",
    "        Pis.append(int(gt * gh * gw))\n",
    "    P_total = sum(Pis)\n",
    "    if P_total != flatten_patches.shape[0]:\n",
    "        raise ValueError(f\"sum(Pis)={P_total} != flatten_patches.shape[0]={flatten_patches.shape[0]}\")\n",
    "\n",
    "    # slice through flatten_patches and invert each\n",
    "    outputs: List[torch.Tensor] = []\n",
    "    start = 0\n",
    "    for i, (gt, gh, gw) in enumerate(grid_vals):\n",
    "        Pi = Pis[i]\n",
    "        slice_i = flatten_patches[start : start + Pi, :]  # differentiable slicing\n",
    "        start += Pi\n",
    "        images_i = _invert_single(\n",
    "            slice_i,\n",
    "            grid_t=int(gt),\n",
    "            grid_h=int(gh),\n",
    "            grid_w=int(gw),\n",
    "            patch_size=patch_size,\n",
    "            temporal_patch_size=temporal_patch_size,\n",
    "            merge_size=merge_size,\n",
    "            channel=channel,\n",
    "            do_unnormalize=do_unnormalize,\n",
    "            image_mean=image_mean,\n",
    "            image_std=image_std,\n",
    "            do_unrescale=do_unrescale,\n",
    "            rescale_factor=rescale_factor,\n",
    "            output_data_format=\"channels_first\",  # produce channels_first for easier padding\n",
    "        )\n",
    "        outputs.append(images_i)\n",
    "\n",
    "    if return_as == \"list\":\n",
    "        # return list of (num_frames_i, C, H_i, W_i)\n",
    "        if output_data_format == \"channels_last\":\n",
    "            # convert each to channels_last\n",
    "            outputs = [img.permute(0, 2, 3, 1) for img in outputs]\n",
    "        return outputs\n",
    "\n",
    "    # else return padded tensor\n",
    "    # compute max dims\n",
    "    num_frames_list = [img.shape[0] for img in outputs]\n",
    "    H_list = [img.shape[2] for img in outputs]\n",
    "    W_list = [img.shape[3] for img in outputs]\n",
    "    max_frames = max(num_frames_list)\n",
    "    max_H = max(H_list)\n",
    "    max_W = max(W_list)\n",
    "\n",
    "    padded = torch.zeros((N, max_frames, channel, max_H, max_W), dtype=dtype, device=device)\n",
    "    mask = torch.zeros((N, max_frames), dtype=torch.bool, device=device)\n",
    "\n",
    "    for i, img in enumerate(outputs):\n",
    "        nf, c, h, w = img.shape\n",
    "        padded[i, :nf, :c, :h, :w] = img  # fills with differentiable ops (assignment to a view) - still OK for autograd\n",
    "        mask[i, :nf] = True\n",
    "\n",
    "    if output_data_format == \"channels_last\":\n",
    "        # permute to (N, max_frames, max_H, max_W, C)\n",
    "        padded = padded.permute(0, 1, 3, 4, 2)\n",
    "\n",
    "    return padded, mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf27169",
   "metadata": {},
   "source": [
    "## 尝试从image token中恢复图片"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf1117b",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_pixels = out[\"pixel_values\"]\n",
    "print(img_pixels.shape)\n",
    "img_thw = out[\"image_grid_thw\"]\n",
    "print(img_thw)\n",
    "out1 = invert_patches_to_images_batch(\n",
    "    flatten_patches=img_pixels,\n",
    "    image_grid_thw=img_thw,\n",
    "    patch_size=16,\n",
    "    temporal_patch_size=2,\n",
    "    merge_size=2,\n",
    ")\n",
    "print(out1[0].shape)\n",
    "ori_size = out1[0].shape\n",
    "images = out1[0].reshape(-1,ori_size[2],ori_size[3],ori_size[4])\n",
    "# Generate a grid with 5 images per row\n",
    "grid = torchvision.utils.make_grid(images, normalize=True, nrow=4)\n",
    "\n",
    "# Convert the grid to a format suitable for matplotlib\n",
    "plt.imshow(grid.permute(1, 2, 0)) # Move channels to the last dimension\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44375928-68cc-402a-920f-b2f1663ba4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bchw_to_pixel_values(\n",
    "    images: torch.Tensor,\n",
    "    patch_size: int=16,\n",
    "    temporal_patch_size: int=2,\n",
    "    merge_size: int=2,\n",
    ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Per-image preprocess and concat flatten patches.\n",
    "\n",
    "    Args:\n",
    "        images: torch.Tensor (B, C, H, W)\n",
    "        patch_size: spatial patch size\n",
    "        temporal_patch_size: number of temporal frames to group (>=1)\n",
    "        merge_size: merge factor\n",
    "\n",
    "    Returns:\n",
    "        pixel_values: torch.Tensor (N_total_patches, vector_len)\n",
    "        image_grid_thw: torch.LongTensor (B, 3) with rows [grid_t, grid_w, grid_h]\n",
    "                         (matches the sample format: [1, 30, 40])\n",
    "    Notes:\n",
    "        - Assumes H % patch_size == 0 and W % patch_size == 0.\n",
    "        - Assumes grid_h % merge_size == 0 and grid_w % merge_size == 0.\n",
    "        - Each image is processed independently; temporal padding is done per-image\n",
    "          by repeating the last frame to reach a multiple of temporal_patch_size.\n",
    "        - All operations are differentiable (torch ops).\n",
    "    \"\"\"\n",
    "    if images.ndim != 4:\n",
    "        raise ValueError(\"images must have shape (B, C, H, W)\")\n",
    "\n",
    "    B, C, H, W = images.shape\n",
    "    if patch_size <= 0 or temporal_patch_size <= 0 or merge_size <= 0:\n",
    "        raise ValueError(\"patch_size, temporal_patch_size and merge_size must be > 0\")\n",
    "\n",
    "    if H % patch_size != 0 or W % patch_size != 0:\n",
    "        raise ValueError(\"H and W must be divisible by patch_size\")\n",
    "\n",
    "    grid_h = H // patch_size\n",
    "    grid_w = W // patch_size\n",
    "\n",
    "    if grid_h % merge_size != 0 or grid_w % merge_size != 0:\n",
    "        raise ValueError(\"grid_h and grid_w must be divisible by merge_size\")\n",
    "\n",
    "    per_image_patches = []   # list of tensors (num_patches_i, vec_len)\n",
    "    grid_list = []           # list of [grid_t, grid_w, grid_h] per image (match your sample)\n",
    "\n",
    "    # For each image in batch, treat it as a sequence of 1 frame and pad temporally to temporal_patch_size.\n",
    "    for i in range(B):\n",
    "        img = images[i]  # (C, H, W)\n",
    "\n",
    "        # Make a \"temporal\" dimension by stacking/repeating this single frame so we have B_i frames\n",
    "        # Start with single frame count = 1\n",
    "        frames = img.unsqueeze(0)  # (1, C, H, W)\n",
    "        if frames.shape[0] % temporal_patch_size != 0:\n",
    "            pad_n = temporal_patch_size - (frames.shape[0] % temporal_patch_size)\n",
    "            last = frames[-1:].expand(pad_n, -1, -1, -1).contiguous()\n",
    "            frames = torch.cat([frames, last], dim=0)  # (temporal_patch_size, C, H, W)\n",
    "\n",
    "        B_i = frames.shape[0]  # will be multiple of temporal_patch_size\n",
    "        grid_t = B_i // temporal_patch_size\n",
    "\n",
    "        # Now we have frames shaped (B_i, C, H, W)\n",
    "        # reshape to expose patch dims:\n",
    "        # target reshape similar to numpy code:\n",
    "        # (grid_t, temporal_patch_size, C, grid_h//merge_size, merge_size, patch_size, grid_w//merge_size, merge_size, patch_size)\n",
    "        # first view (grid_t, temporal_patch_size, C, grid_h, patch_size, grid_w, patch_size)\n",
    "        # then split grid dims by merge_size and permute like original algorithm\n",
    "\n",
    "        # Step1: reshape H and W into (grid_h, patch_size) and (grid_w, patch_size)\n",
    "        frames_view = frames.view(\n",
    "            grid_t,\n",
    "            temporal_patch_size,\n",
    "            C,\n",
    "            grid_h,\n",
    "            patch_size,\n",
    "            grid_w,\n",
    "            patch_size,\n",
    "        )  # shape: (grid_t, t_p, C, g_h, p, g_w, p)\n",
    "\n",
    "        # Step2: split grid_h/grid_w by merge_size: (g_h//merge_size, merge_size)\n",
    "        frames_view = frames_view.view(\n",
    "            grid_t,\n",
    "            temporal_patch_size,\n",
    "            C,\n",
    "            grid_h // merge_size,\n",
    "            merge_size,\n",
    "            patch_size,\n",
    "            grid_w // merge_size,\n",
    "            merge_size,\n",
    "            patch_size,\n",
    "        )  # (grid_t, t_p, C, gh_m, m1, p, gw_m, m2, p)\n",
    "\n",
    "        # Permute to match numpy ordering: transpose(0, 3, 6, 4, 7, 2, 1, 5, 8)\n",
    "        perm = frames_view.permute(0, 3, 6, 4, 7, 2, 1, 5, 8).contiguous()\n",
    "        # shape now: (grid_t, gh_m, gw_m, m1, m2, C, t_p, p, p)\n",
    "\n",
    "        # Combine back gh_m * m1 -> grid_h, gw_m * m2 -> grid_w\n",
    "        # new view: (grid_t, grid_h, grid_w, C, t_p, p, p)\n",
    "        perm_view = perm.view(\n",
    "            grid_t,\n",
    "            grid_h,\n",
    "            grid_w,\n",
    "            C,\n",
    "            temporal_patch_size,\n",
    "            patch_size,\n",
    "            patch_size,\n",
    "        ).contiguous()\n",
    "\n",
    "        # Finally flatten patch-vector dims: (grid_t * grid_h * grid_w, C * temporal_patch_size * patch_size * patch_size)\n",
    "        num_patches_i = grid_t * grid_h * grid_w\n",
    "        vec_len = C * temporal_patch_size * patch_size * patch_size\n",
    "        patches_i = perm_view.view(num_patches_i, vec_len)\n",
    "\n",
    "        per_image_patches.append(patches_i)\n",
    "        # IMPORTANT: user sample shows image_grid_thw as [[1,30,40]], i.e. order (grid_t, grid_w, grid_h)\n",
    "        grid_list.append([grid_t, grid_w, grid_h])\n",
    "\n",
    "    # Concatenate all images' patches along rows (first dimension)\n",
    "    pixel_values = torch.cat(per_image_patches, dim=0)  # (sum num_patches_i, vec_len)\n",
    "    image_grid_thw = torch.tensor(grid_list, dtype=torch.long, device=pixel_values.device)  # (B, 3)\n",
    "\n",
    "    return pixel_values, image_grid_thw\n",
    "    \n",
    "new_batch = images[[0,2],...]\n",
    "print(f\"New batch size {new_batch.shape}\")\n",
    "new_patches, new_thw = bchw_to_pixel_values(new_batch)\n",
    "print(\"Cvt batches:\",new_patches.shape, new_thw)\n",
    "print(f\"ERROR: {torch.sum(torch.abs(new_patches-img_pixels))}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
